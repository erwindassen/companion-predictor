{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import dask\n",
    "# from dask import array as da\n",
    "from dask import dataframe as dd\n",
    "# from dask import delayed\n",
    "# from dask.multiprocessing import get\n",
    "import pandas as pd\n",
    "import pathlib2 as pl\n",
    "import mmh3  # The hash function used to hash sites. See the preprocessor script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# dask.set_options(get=get);  # Due to a bug we can't read files in different processes so set this option after reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessor supports output into `numpy` `arrays` and `pandas` `DataFrames` and `scikit-learn` supports the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = int(1e5)\n",
    "DF_DIR = pl.Path('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00*.hdf')\n",
    "str(DF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = dd.read_hdf(str(DF_DIR), key='dataset', chunksize=CHUNK_SIZE)\n",
    "data = pd.read_hdf('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00_0-200_20160624101922.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datetime_index = data.index.get_level_values('datetime_start')\n",
    "datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "data['day'] = datetime_index.to_period(freq='d')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a query at this stage to limit the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, test and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split by selecting a day as test and another as validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days = data['day'].value_counts()\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_day = days.keys()[-2]\n",
    "validation_day = days.keys()[-3]\n",
    "print(test_day, validation_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_for_day(data, day, complement=False):\n",
    "    datetime_index = data.index.get_level_values('datetime_start')\n",
    "    datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "    if complement:\n",
    "        return data[(datetime_index.year != day.year) | (datetime_index.month != day.month) | (datetime_index.day != day.day)]\n",
    "    else:\n",
    "        return data[(datetime_index.year == day.year) & (datetime_index.month == day.month) & (datetime_index.day == day.day)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = filter_for_day(data, test_day)\n",
    "validation_data = filter_for_day(data, validation_day)\n",
    "train_data = filter_for_day(data, test_day, complement=True)  # Exclude the test day...\n",
    "train_data = filter_for_day(train_data, validation_day, complement=True)  # ... and the validation day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape, train_data.shape, test_data.shape, validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['site_hash', 'timestamp_start', 'precipitation mm/h', 'temperature C', 'windspeed m/s']\n",
    "targets = ['trafficspeed km/h']#, 'trafficflow counts/h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `site_hash` is the `mmh3.hash64` of the `site` column (the last component actually):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmh3.hash64('rws01_monibas_0010vwa0056ra')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features.npartitions  # Only for dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we (lazy) loaded the entire dataset. It has been distributed into the above number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne  # Ignore any errors for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "# theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model one sequence as whole-day measurement of a site. Batches are sets of such sequences of (possibly) various sizes. We expect a periodicity on the day level and try to fit a model to such behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate mini batches with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from random import shuffle\n",
    "\n",
    "def batches(source_df, sites=None, days=None, max_batches=1000, max_batch_length=100):\n",
    "    if sites is None:\n",
    "        site_bag = set(source_df.index.get_level_values(0))\n",
    "        \n",
    "    if days is None:\n",
    "        day_bag = set(source_df['day'].unique())\n",
    "        \n",
    "    sample_bag = product(sites, days)\n",
    "#     sample_bag = list(sample_bag)  # Takes too long\n",
    "#     shuffle(sample_bag)  # Takes too long\n",
    "\n",
    "    for i in range(max_batches):\n",
    "        samples = list()\n",
    "        for j in range(max_batch_length):\n",
    "            try:\n",
    "                samples.append(next(sample_bag))\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        if len(samples) == 0:\n",
    "#             print(\"No samples at batch %i\" % i)\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Prepare batch\n",
    "        batch_length = len(samples)\n",
    "        batch = np.zeros([batch_length, max_seq_length, len(features)], dtype='float64')\n",
    "        mask = np.zeros([batch_length, max_seq_length], dtype='float64')\n",
    "        target = np.zeros([batch_length, max_seq_length, len(targets)], dtype='float64')\n",
    "        \n",
    "        for j in range(batch_length):\n",
    "            site, period = samples[j]\n",
    "            \n",
    "            # query measurements\n",
    "            data = source_df.query(\"site == '%s'\" % site)\n",
    "            data = data[data['day'] == period]\n",
    "            \n",
    "            data_f = data[features].values\n",
    "            data_t = data[targets].values\n",
    "\n",
    "            seq_length = data.shape[0]\n",
    "            assert seq_length <= max_seq_length, \"Error: sequence longer than `max_seq_length` found\"\n",
    "\n",
    "            batch[j, :seq_length, :] = data_f\n",
    "            target[j, :seq_length, :] = data_t\n",
    "            mask[j, :seq_length] = np.ones([seq_length])\n",
    "            \n",
    "        yield i, batch, target, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the batch generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_bag = set(train_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdt = pd.DatetimeIndex(train_data.index.get_level_values('datetime_start'))\n",
    "day_bag = set(tdt.to_period(freq='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_batches = 1000\n",
    "max_batch_length = 15  # Maximum number os day-long measurement sequence (of one site) per batch\n",
    "\n",
    "max_seq_length = 24*60 # Maximum number of measurements per site per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for batch_num, batch, target, mask in batches(train_data, sites=site_bag, days=day_bag):\n",
    "#     print(batch_num, batch.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor3('input', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_values = T.tensor3('target', dtype=theano.config.floatX)\n",
    "# target_values = T.matrix('target', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None, 5), input_var=input_var, name='input_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_mask = lasagne.layers.InputLayer(shape=(None,None), name='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lasagne.layers.LSTMLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_lstm_units = 20\n",
    "max_grad = 5.0\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_in, num_units=num_lstm_units,\n",
    "                                  gradient_steps=-1, grad_clipping=max_grad, unroll_scan=False,\n",
    "                                  mask_input=l_mask, name='l_lstm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to combine the LSTM with a dense layer and need to reshape the input. We dot this with a `ReshapeLayer`\n",
    "help(lasagne.layers.ReshapeLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, retrieve symbolic variables for the input shape\n",
    "n_batch, n_time_steps, n_features = l_in.input_var.shape\n",
    "\n",
    "# Now, squash the n_batch and n_time_steps dimensions\n",
    "l_reshape_in = lasagne.layers.ReshapeLayer(l_lstm, (-1, num_lstm_units)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we can apply feed-forward layers as usual.\n",
    "l_dense_1 = lasagne.layers.DenseLayer(l_reshape_in, num_units=20, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_1')\n",
    "l_dense_2 = lasagne.layers.DenseLayer(l_dense_1, num_units=1, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_2')\n",
    "# Now, the shape will be n_batch*n_timesteps, 1.  We can then reshape to\n",
    "# n_batch, n_timesteps to get a single value for each timstep from each sequence\n",
    "l_reshape_out = lasagne.layers.ReshapeLayer(l_dense_2, (n_batch, n_time_steps, 1), name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_reshape_out)\n",
    "# The value we care about is the final value produced for each sequence\n",
    "# so we simply slice it out.\n",
    "predicted_values = network_output#[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our cost will be mean-squared error\n",
    "# help(lasagne.objectives.squared_error)\n",
    "loss = T.mean(lasagne.objectives.squared_error(predicted_values, target_values))\n",
    "# loss = T.mean((predicted_values - target_values)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_reshape_out)\n",
    "# all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute adam updates for training\n",
    "help(lasagne.updates.adam)\n",
    "updates = lasagne.updates.adam(loss, all_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano functions for training computing cost and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function([l_in.input_var, target_values, l_mask.input_var], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost = theano.function([l_in.input_var, target_values, l_mask.input_var], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ide = theano.function([target_values], outputs=[target_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = theano.function([l_in.input_var, l_mask.input_var], outputs=[predicted_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_num, batch, target, mask = next(iter(batches(train_data, sites=site_bag, days=day_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(batch.shape, target.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = ff(batch, mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = ide(target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(batch, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sites = set(test_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_batches = 1000\n",
    "max_batch_length = 100  # Maximum number os day-long measurement sequence (of one site) per batch\n",
    "\n",
    "max_seq_length = 24*60 # Maximum number of measurements per site per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll train the network with 10 epochs of a maximum of `max_batches` each\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('TRAIN', end=' ')\n",
    "    for batch_num, batch, target, mask in batches(train_data, sites=site_bag, days=day_bag,\n",
    "                                                  max_batches=max_batches, max_batch_length=max_batch_length):\n",
    "        train(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            print(\".\", end='')\n",
    "            if batch_num % 100 == 0:\n",
    "                print(batch_num, end='')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    cost_val = 0.0\n",
    "    print('TEST', end=' ')\n",
    "    for batch_num, batch, target, mask in batches(test_data, days=set((test_day,)), sites=test_sites):\n",
    "        cost_val += compute_cost(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            print(batch_num, end='')\n",
    "\n",
    "    cost_val = cost_val/(batch_num + 1)\n",
    "    print('')\n",
    "    \n",
    "    print(\"Epoch {} validation cost = {}\".format(epoch + 1, cost_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "companionenv",
   "language": "python",
   "name": "companionenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
