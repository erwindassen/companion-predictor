{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import dask\n",
    "# from dask import array as da\n",
    "from dask import dataframe as dd\n",
    "# from dask import delayed\n",
    "# from dask.multiprocessing import get\n",
    "import pandas as pd\n",
    "import pathlib2 as pl\n",
    "import mmh3  # The hash function used to hash sites. See the preprocessor script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# dask.set_options(get=get);  # Due to a bug we can't read files in different processes so set this option after reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 26715\n",
    "CHUNK_SIZE = int(2e5)\n",
    "\n",
    "MAX_BATCHES = 1000\n",
    "MAX_BATCH_LENGTH = 100  # Maximum number os day-long measurement sequence (of one site) per batch\n",
    "MAX_SEQ_LENGTH = 24*60 # Maximum number of measurements per site per day\n",
    "SUBSEQ_LENGTH = 60  # 1 measurement/min a subsquence of 1h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessor supports output into `numpy` `arrays` and `pandas` `DataFrames` and `scikit-learn` supports the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00*.hdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_DIR = pl.Path('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00*.hdf')\n",
    "str(DF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dd.read_hdf(str(DF_DIR), key='dataset', chunksize=CHUNK_SIZE)\n",
    "# data = pd.read_hdf('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00_0-200_20160624101922.hdf')\n",
    "# data.head()\n",
    "\n",
    "day_counts = dd.read_hdf(str(DF_DIR), key='day_counts', chunksize=CHUNK_SIZE)\n",
    "site_counts = dd.read_hdf(str(DF_DIR), key='site_counts', chunksize=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rws01_monibas_0010vwa0065ra',\n",
       " 'rws01_monibas_0010vwa0223ra',\n",
       " 'rws01_monibas_0010vwa0248ra',\n",
       " 'rws01_monibas_0010vwa0269ra',\n",
       " 'rws01_monibas_0010vwa0286ra']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../selected_sites.pkl', mode='rb') as fname:\n",
    "    sites = pickle.load(fname)\n",
    "\n",
    "print(len(sites))\n",
    "sites[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample, seed, shuffle\n",
    "seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_sites = sample(sites, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(data))\n",
    "data = data.query('site in {}'.format(samples_sites))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you're using commits of the preprocessor after 10 Aug this is already done for you and you can skip it. \n",
    "# def create_day(data):\n",
    "#     datetime_index = data.index.get_level_values('datetime_start')\n",
    "#     datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "#     data['day'] = datetime_index.to_period(freq='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a query at this stage to limit the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, test and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split by selecting a day as test and another as validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_day = day_counts.\n",
    ".keys()[-2]\n",
    "validation_day = days.keys()[-3]\n",
    "print(test_day, validation_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def filter_for_day(data, day, complement=False):\n",
    "    datetime_index = data.index.get_level_values('datetime_start')\n",
    "    datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "    if complement:\n",
    "        return data[(datetime_index.year != day.year) | (datetime_index.month != day.month) | (datetime_index.day != day.day)]\n",
    "    else:\n",
    "        return data[(datetime_index.year == day.year) & (datetime_index.month == day.month) & (datetime_index.day == day.day)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = filter_for_day(data, test_day)\n",
    "validation_data = filter_for_day(data, validation_day)\n",
    "train_data = filter_for_day(data, test_day, complement=True)  # Exclude the test day...\n",
    "train_data = filter_for_day(train_data, validation_day, complement=True)  # ... and the validation day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape, train_data.shape, test_data.shape, validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['site_hash', 'timestamp_start', 'precipitation mm/h', 'temperature C', 'windspeed m/s']\n",
    "targets = ['trafficspeed km/h']#, 'trafficflow counts/h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `site_hash` is the `mmh3.hash64` of the `site` column (the last component actually):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmh3.hash64('rws01_monibas_0010vwa0056ra')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features.npartitions  # Only for dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we (lazy) loaded the entire dataset. It has been distributed into the above number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "theano.config.exception_verbosity = 'high'\n",
    "theano.config.floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model one sequence as whole-day measurement of a site. Batches are sets of such sequences of (possibly) various sizes. We expect a periodicity on the day level and try to fit a model to such behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To calculate the loss we could take the mean of the squared error over the whole sequence\n",
    "but that would affect the lerning rate considerably. In the literature people tend to take\n",
    "only the last sequence elements. For us *THIS IS NOT IDEAL SINCE OUR WE HAVE DAY-LONG MEASUREMENTS*\n",
    "and the last element is with high probability around the end of the day. We would overwhelmingly train\n",
    "to predict end of day traffic measurements.\n",
    "\n",
    "We solve this by batching over *random* subsequences of the whole day sequences with a length controlled\n",
    "by SUBSEQ_LENGTH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate mini batches with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def batches(source_df, sites=None, days=None,\n",
    "            max_batches=MAX_BATCHES, max_batch_length=MAX_BATCH_LENGTH, subseq_length=SUBSEQ_LENGTH):\n",
    "    if sites is None:\n",
    "        site_bag = set(source_df.index.get_level_values(0))\n",
    "        \n",
    "    if days is None:\n",
    "        day_bag = set(source_df['day'].unique())\n",
    "        \n",
    "    sample_bag = product(sites, days)\n",
    "\n",
    "    for i in range(max_batches):\n",
    "        samples = list()\n",
    "        for j in range(max_batch_length):\n",
    "            try:\n",
    "                samples.append(next(sample_bag))\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        if len(samples) == 0:\n",
    "#             print(\"No samples at batch %i\" % i)\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Prepare batch\n",
    "        batch_length = len(samples)\n",
    "        batch = np.zeros([batch_length, subseq_length, len(features)], dtype='float64')\n",
    "        mask = np.zeros([batch_length, subseq_length], dtype='float64')\n",
    "        target = np.zeros([batch_length, subseq_length, len(targets)], dtype='float64')\n",
    "\n",
    "        max_seq_length = 0\n",
    "        for j in range(batch_length):\n",
    "            site, period = samples[j]\n",
    "            \n",
    "            # query measurements\n",
    "            data = source_df.query(\"site == '%s'\" % site)\n",
    "            data = data[data['day'] == period]\n",
    "            \n",
    "            data_f = data[features].values\n",
    "            data_t = data[targets].values\n",
    "\n",
    "            this_length = len(data)\n",
    "            max_seq_length = max(max_seq_length, this_length)\n",
    "            if this_length < subseq_length:\n",
    "#                 print('Warning: batch to small: {}'.format(this_length))\n",
    "\n",
    "                batch[j, :this_length, :] = data_f\n",
    "                target[j, :this_length, :] = data_t\n",
    "                mask[j, :this_length] = np.ones([this_length])\n",
    "            else:\n",
    "                # Select a random subsequence\n",
    "                ind = np.random.random_integers(0, high=(this_length-subseq_length))\n",
    "                data_f = data_f[ind:ind+subseq_length,:]\n",
    "                data_t = data_t[ind:ind+subseq_length,:]\n",
    "\n",
    "                batch[j, :, :] = data_f\n",
    "                target[j, :, :] = data_t\n",
    "                mask[j, :] = np.ones([subseq_length])\n",
    "        \n",
    "#         print('Max sequence length in this batch: {}'.format(max_seq_length))\n",
    "        yield i, batch, target, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor3('input', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_tensor = T.tensor3('target', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None, len(features)), input_var=input_var, name='input_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_mask = lasagne.layers.InputLayer(shape=(None,None), name='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lasagne.layers.LSTMLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_lstm_units = 64\n",
    "max_grad = 100.0\n",
    "l_lstm_1 = lasagne.layers.LSTMLayer(l_in, num_units=num_lstm_units,\n",
    "                                    hid_init=lasagne.init.GlorotUniform(), cell_init=lasagne.init.GlorotNormal(),\n",
    "                                    gradient_steps=-1, grad_clipping=max_grad, unroll_scan=False,\n",
    "                                    mask_input=l_mask, name='l_lstm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_lstm_2 = lasagne.layers.LSTMLayer(l_lstm_1, num_units=num_lstm_units//4,\n",
    "                                    hid_init=lasagne.init.GlorotUniform(), cell_init=lasagne.init.GlorotNormal(),\n",
    "                                    gradient_steps=-1, grad_clipping=max_grad, unroll_scan=False,\n",
    "                                    mask_input=l_mask, name='l_lstm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_reshape_out = lasagne.layers.LSTMLayer(l_lstm_2, num_units=1,\n",
    "                                         hid_init=lasagne.init.GlorotUniform(), cell_init=lasagne.init.GlorotNormal(),\n",
    "                                         gradient_steps=-1, grad_clipping=max_grad, unroll_scan=False,\n",
    "                                         mask_input=l_mask, name='l_lstm_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is no longer in use but here nontheless for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to combine the LSTM with a dense layer and need to reshape the input. We dot this with a `ReshapeLayer`\n",
    "# help(lasagne.layers.ReshapeLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we can apply feed-forward layers as usual.\n",
    "# l_dense_1 = lasagne.layers.DenseLayer(l_reshape_in, num_units=num_lstm_units, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_1')\n",
    "# l_dense_2 = lasagne.layers.DenseLayer(l_dense_1, num_units=1, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_2')\n",
    "# Now, the shape will be n_batch*n_timesteps, 1.  We can then reshape to\n",
    "# n_batch, n_timesteps to get a single value for each timstep from each sequence\n",
    "# l_reshape_out = lasagne.layers.ReshapeLayer(l_dense_2, (n_batch, n_time_steps, 1), name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lasagne.layers.get_output_shape(l_lstm_1))\n",
    "print(lasagne.layers.get_output_shape(l_lstm_2))\n",
    "\n",
    "print(lasagne.layers.get_output_shape(l_reshape_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_reshape_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_values = T.squeeze(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We do the same for the target_values\n",
    "target_values = T.squeeze(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our cost will be mean-squared error\n",
    "# help(lasagne.objectives.squared_error)\n",
    "loss = T.mean(lasagne.objectives.squared_error(predicted_values, target_values))\n",
    "# loss = (predicted_values - target_values)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_reshape_out)\n",
    "# all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute adam updates for training\n",
    "updates = lasagne.updates.adam(loss, all_params, learning_rate=0.01)\n",
    "# updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano functions for training computing cost and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function([l_in.input_var, target_tensor, l_mask.input_var], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost = theano.function([l_in.input_var, target_tensor, l_mask.input_var], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ide = theano.function([target_tensor], outputs=[target_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the feed-forward functions used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = theano.function([l_in.input_var, l_mask.input_var], outputs=[predicted_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_sites = set(validation_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_num, batch, target, mask = next(iter(batches(validation_data, sites=validation_sites, days=(validation_day,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(batch.shape, target.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = ff(batch, mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o[50,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = ide(target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost(batch, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take a long time beware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_bag = list(set(train_data.index.get_level_values(0)))  # set gets rid of the duplicates\n",
    "test_sites = list(set(test_data.index.get_level_values(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_bag = list(train_data['day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(site_bag), len(day_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll train the network with 10 epochs of a maximum of `max_batches` each\n",
    "# Note the shuffle in each iteration of a batch\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('TRAIN Epoch {}'.format(epoch), end=' ')\n",
    "    shuffle(site_bag)\n",
    "    shuffle(day_bag)\n",
    "    for batch_num, batch, target, mask in batches(train_data, sites=site_bag, days=day_bag):\n",
    "        train(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            if batch_num % 100 == 0:\n",
    "                print(batch_num, end='')\n",
    "            print(\".\", end='')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    cost_test = 0.0\n",
    "    print('TEST Epoch {}'.format(epoch), end=' ')\n",
    "    for batch_num, batch, target, mask in batches(test_data, days=set((test_day,)), sites=test_sites):\n",
    "        cost_test += compute_cost(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            print(batch_num, end='')\n",
    "\n",
    "    cost_test = cost_test/(batch_num + 1)\n",
    "    print('')\n",
    "    \n",
    "    print(\"Epoch {} test cost (RMSE) = {}\".format(epoch + 1, sqrt(cost_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(fpath):\n",
    "    np.savez(fpath, *lasagne.layers.get_all_param_values(l_reshape_out))\n",
    "\n",
    "def load(fpath):\n",
    "    with np.load(fpath) as f:\n",
    "        all_param = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(l_reshape_out, all_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save('../src/predictor/models/lasagne_lstm_adam_0.01.npz')  # Give an identifying name please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load('../src/predictor/models/lasagne_lstm_adam_0.01.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.d3viz as d3v\n",
    "from IPython.display import IFrame\n",
    "d3v.d3viz(ff, '../tf_logs/lasagne_lstm.html')\n",
    "IFrame('../tf_logs/lasagne_lstm.html', width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_sites = set(validation_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_num, batch, target, mask = next(iter(batches(validation_data, sites=validation_sites, days=(validation_day,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost(batch, target, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_sample = batch[60,:720,:]\n",
    "seq_sample_target = target[60,:720,:]\n",
    "seq_sample_mask = mask[60,:720]\n",
    "print(seq_sample.shape)\n",
    "print(seq_sample_target[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff(seq_sample.reshape((1, *seq_sample.shape)), seq_sample_mask.reshape((1, *seq_sample_mask.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "companionenv",
   "language": "python",
   "name": "companionenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
