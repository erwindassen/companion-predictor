{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import dask\n",
    "# from dask import array as da\n",
    "from dask import dataframe as dd\n",
    "# from dask import delayed\n",
    "# from dask.multiprocessing import get\n",
    "import pandas as pd\n",
    "import pathlib2 as pl\n",
    "import mmh3  # The hash function used to hash sites. See the preprocessor script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# dask.set_options(get=get);  # Due to a bug we can't read files in different processes so set this option after reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessor supports output into `numpy` `arrays` and `pandas` `DataFrames` and `scikit-learn` supports the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00*.hdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_SIZE = int(2e5)\n",
    "DF_DIR = pl.Path('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00*.hdf')\n",
    "str(DF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precipitation mm/h</th>\n",
       "      <th>temperature C</th>\n",
       "      <th>timestamp_start</th>\n",
       "      <th>...</th>\n",
       "      <th>trafficspeed km/h</th>\n",
       "      <th>windspeed m/s</th>\n",
       "      <th>site_hash</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">rws01_monibas_0010vwa0056ra</th>\n",
       "      <th>2016-05-23 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1464044400</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1854210412049763327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>1464048000</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1854210412049763327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1464051600</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1854210412049763327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1464055200</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1854210412049763327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>1464058800</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1854210412049763327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 precipitation mm/h  temperature C  timestamp_start         ...           trafficspeed km/h  windspeed m/s            site_hash\n",
       "site                        datetime_start                                                                  ...                                                                \n",
       "rws01_monibas_0010vwa0056ra 2016-05-23 23:00:00                 0.0           10.4       1464044400         ...                        55.0            3.0 -1854210412049763327\n",
       "                            2016-05-24 00:00:00                 0.0           10.3       1464048000         ...                        55.0            6.0 -1854210412049763327\n",
       "                            2016-05-24 01:00:00                 0.0           10.2       1464051600         ...                        55.0            4.0 -1854210412049763327\n",
       "                            2016-05-24 02:00:00                 0.0           10.1       1464055200         ...                        55.0            4.0 -1854210412049763327\n",
       "                            2016-05-24 03:00:00                 0.0           10.3       1464058800         ...                        55.0            3.0 -1854210412049763327\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = dd.read_hdf(str(DF_DIR), key='dataset', chunksize=CHUNK_SIZE)\n",
    "data = pd.read_hdf('/Volumes/CompanionEx/Data/dfs_pandas/PP_TS_2016-05-24-00_2016-06-01-00_0-200_20160624101922.hdf')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rws01_monibas_0010vwa0065ra',\n",
       " 'rws01_monibas_0010vwa0223ra',\n",
       " 'rws01_monibas_0010vwa0248ra',\n",
       " 'rws01_monibas_0010vwa0269ra',\n",
       " 'rws01_monibas_0010vwa0286ra']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../selected_sites.pkl', mode='rb') as fname:\n",
    "    sites = pickle.load(fname)\n",
    "\n",
    "print(len(sites))\n",
    "sites[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_sites = sample(sites, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3790662\n",
      "426024\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "data = data.query('site in @samples_sites')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precipitation mm/h</th>\n",
       "      <th>temperature C</th>\n",
       "      <th>timestamp_start</th>\n",
       "      <th>...</th>\n",
       "      <th>windspeed m/s</th>\n",
       "      <th>site_hash</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">rws01_monibas_0010vwa0065ra</th>\n",
       "      <th>2016-05-23 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1464044400</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8446559662630338889</td>\n",
       "      <td>2016-05-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>1464048000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8446559662630338889</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1464051600</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8446559662630338889</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1464055200</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8446559662630338889</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>1464058800</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8446559662630338889</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 precipitation mm/h  temperature C  timestamp_start    ...      windspeed m/s            site_hash        day\n",
       "site                        datetime_start                                                             ...                                                   \n",
       "rws01_monibas_0010vwa0065ra 2016-05-23 23:00:00                 0.0           10.4       1464044400    ...                3.0  8446559662630338889 2016-05-23\n",
       "                            2016-05-24 00:00:00                 0.0           10.3       1464048000    ...                6.0  8446559662630338889 2016-05-24\n",
       "                            2016-05-24 01:00:00                 0.0           10.2       1464051600    ...                4.0  8446559662630338889 2016-05-24\n",
       "                            2016-05-24 02:00:00                 0.0           10.1       1464055200    ...                4.0  8446559662630338889 2016-05-24\n",
       "                            2016-05-24 03:00:00                 0.0           10.3       1464058800    ...                3.0  8446559662630338889 2016-05-24\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_index = data.index.get_level_values('datetime_start')\n",
    "datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "data['day'] = datetime_index.to_period(freq='d')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a query at this stage to limit the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, test and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split by selecting a day as test and another as validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-05-24    230472\n",
       "2016-05-30     27936\n",
       "2016-05-29     27936\n",
       "2016-05-28     27936\n",
       "2016-05-27     27936\n",
       "2016-05-26     27936\n",
       "2016-05-25     27936\n",
       "2016-05-31     26772\n",
       "2016-05-23      1164\n",
       "Freq: D, Name: day, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = data['day'].value_counts()\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-05-31 2016-05-25\n"
     ]
    }
   ],
   "source": [
    "test_day = days.keys()[-2]\n",
    "validation_day = days.keys()[-3]\n",
    "print(test_day, validation_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_for_day(data, day, complement=False):\n",
    "    datetime_index = data.index.get_level_values('datetime_start')\n",
    "    datetime_index = pd.DatetimeIndex(datetime_index)\n",
    "\n",
    "    if complement:\n",
    "        return data[(datetime_index.year != day.year) | (datetime_index.month != day.month) | (datetime_index.day != day.day)]\n",
    "    else:\n",
    "        return data[(datetime_index.year == day.year) & (datetime_index.month == day.month) & (datetime_index.day == day.day)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = filter_for_day(data, test_day)\n",
    "validation_data = filter_for_day(data, validation_day)\n",
    "train_data = filter_for_day(data, test_day, complement=True)  # Exclude the test day...\n",
    "train_data = filter_for_day(train_data, validation_day, complement=True)  # ... and the validation day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426024, 8) (371316, 8) (26772, 8) (27936, 8)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, train_data.shape, test_data.shape, validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['site_hash', 'timestamp_start', 'precipitation mm/h', 'temperature C', 'windspeed m/s']\n",
    "targets = ['trafficspeed km/h']#, 'trafficflow counts/h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `site_hash` is the `mmh3.hash64` of the `site` column (the last component actually):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmh3.hash64('rws01_monibas_0010vwa0056ra')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features.npartitions  # Only for dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we (lazy) loaded the entire dataset. It has been distributed into the above number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "# theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/companionenv/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne  # Ignore any errors for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model one sequence as whole-day measurement of a site. Batches are sets of such sequences of (possibly) various sizes. We expect a periodicity on the day level and try to fit a model to such behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate mini batches with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from random import shuffle\n",
    "\n",
    "def batches(source_df, sites=None, days=None, max_batches=1000, max_batch_length=100):\n",
    "    if sites is None:\n",
    "        site_bag = set(source_df.index.get_level_values(0))\n",
    "        \n",
    "    if days is None:\n",
    "        day_bag = set(source_df['day'].unique())\n",
    "        \n",
    "    sample_bag = product(sites, days)\n",
    "#     sample_bag = list(sample_bag)  # Takes too long\n",
    "#     shuffle(sample_bag)  # Takes too long\n",
    "\n",
    "    for i in range(max_batches):\n",
    "        samples = list()\n",
    "        for j in range(max_batch_length):\n",
    "            try:\n",
    "                samples.append(next(sample_bag))\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        if len(samples) == 0:\n",
    "#             print(\"No samples at batch %i\" % i)\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Prepare batch\n",
    "        batch_length = len(samples)\n",
    "        batch = np.zeros([batch_length, max_seq_length, len(features)], dtype='float64')\n",
    "        mask = np.zeros([batch_length, max_seq_length], dtype='float64')\n",
    "        target = np.zeros([batch_length, max_seq_length, len(targets)], dtype='float64')\n",
    "        \n",
    "        for j in range(batch_length):\n",
    "            site, period = samples[j]\n",
    "            \n",
    "            # query measurements\n",
    "            data = source_df.query(\"site == '%s'\" % site)\n",
    "            data = data[data['day'] == period]\n",
    "            \n",
    "            data_f = data[features].values\n",
    "            data_t = data[targets].values\n",
    "\n",
    "            seq_length = data.shape[0]\n",
    "            assert seq_length <= max_seq_length, \"Error: sequence longer than `max_seq_length` found\"\n",
    "\n",
    "            batch[j, :seq_length, :] = data_f\n",
    "            target[j, :seq_length, :] = data_t\n",
    "            mask[j, :seq_length] = np.ones([seq_length])\n",
    "            \n",
    "        yield i, batch, target, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the batch generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_bag = set(train_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdt = pd.DatetimeIndex(train_data.index.get_level_values('datetime_start'))\n",
    "day_bag = set(tdt.to_period(freq='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_batches = 1000\n",
    "max_batch_length = 15  # Maximum number os day-long measurement sequence (of one site) per batch\n",
    "\n",
    "max_seq_length = 24*60 # Maximum number of measurements per site per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for batch_num, batch, target, mask in batches(train_data, sites=site_bag, days=day_bag):\n",
    "#     print(batch_num, batch.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor3('input', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_values = T.tensor3('target', dtype=theano.config.floatX)\n",
    "# target_values = T.matrix('target', dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None, 5), input_var=input_var, name='input_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_mask = lasagne.layers.InputLayer(shape=(None,None), name='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lasagne.layers.LSTMLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_lstm_units = 20\n",
    "max_grad = 5.0\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_in, num_units=num_lstm_units,\n",
    "                                  gradient_steps=-1, grad_clipping=max_grad, unroll_scan=False,\n",
    "                                  mask_input=l_mask, name='l_lstm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to combine the LSTM with a dense layer and need to reshape the input. We dot this with a `ReshapeLayer`\n",
    "help(lasagne.layers.ReshapeLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, retrieve symbolic variables for the input shape\n",
    "n_batch, n_time_steps, n_features = l_in.input_var.shape\n",
    "\n",
    "# Now, squash the n_batch and n_time_steps dimensions\n",
    "l_reshape_in = lasagne.layers.ReshapeLayer(l_lstm, (-1, num_lstm_units)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we can apply feed-forward layers as usual.\n",
    "l_dense_1 = lasagne.layers.DenseLayer(l_reshape_in, num_units=20, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_1')\n",
    "l_dense_2 = lasagne.layers.DenseLayer(l_dense_1, num_units=1, nonlinearity=lasagne.nonlinearities.tanh, name='l_dense_2')\n",
    "# Now, the shape will be n_batch*n_timesteps, 1.  We can then reshape to\n",
    "# n_batch, n_timesteps to get a single value for each timstep from each sequence\n",
    "l_reshape_out = lasagne.layers.ReshapeLayer(l_dense_2, (n_batch, n_time_steps, 1), name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_reshape_out)\n",
    "# The value we care about is the final value produced for each sequence\n",
    "# so we simply slice it out.\n",
    "predicted_values = network_output#[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our cost will be mean-squared error\n",
    "# help(lasagne.objectives.squared_error)\n",
    "loss = T.mean(lasagne.objectives.squared_error(predicted_values, target_values))\n",
    "# loss = T.mean((predicted_values - target_values)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_reshape_out)\n",
    "# all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function adam in module lasagne.updates:\n",
      "\n",
      "adam(loss_or_grads, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
      "    Adam updates\n",
      "    \n",
      "    Adam updates implemented as in [1]_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    loss_or_grads : symbolic expression or list of expressions\n",
      "        A scalar loss expression, or a list of gradient expressions\n",
      "    params : list of shared variables\n",
      "        The variables to generate update expressions for\n",
      "    learning_rate : float\n",
      "        Learning rate\n",
      "    beta_1 : float\n",
      "        Exponential decay rate for the first moment estimates.\n",
      "    beta_2 : float\n",
      "        Exponential decay rate for the second moment estimates.\n",
      "    epsilon : float\n",
      "        Constant for numerical stability.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    OrderedDict\n",
      "        A dictionary mapping each parameter to its update expression\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The paper [1]_ includes an additional hyperparameter lambda. This is only\n",
      "    needed to prove convergence of the algorithm and has no practical use\n",
      "    (personal communication with the authors), it is therefore omitted here.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
      "           Adam: A Method for Stochastic Optimization.\n",
      "           arXiv preprint arXiv:1412.6980.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute adam updates for training\n",
    "help(lasagne.updates.adam)\n",
    "updates = lasagne.updates.adam(loss, all_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano functions for training computing cost and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/envs/companionenv/lib/python3.5/site-packages/theano/gof/cc.py:948: UserWarning: Your g++ compiler fails to compile OpenMP code. We know this happen with some version of the EPD mingw compiler and LLVM compiler on Mac OS X. We disable openmp everywhere in Theano. To remove this warning set the theano flags `openmp` to False.\n",
      "  ret += x.c_compile_args()\n"
     ]
    }
   ],
   "source": [
    "train = theano.function([l_in.input_var, target_values, l_mask.input_var], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost = theano.function([l_in.input_var, target_values, l_mask.input_var], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ide = theano.function([target_values], outputs=[target_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = theano.function([l_in.input_var, l_mask.input_var], outputs=[predicted_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_num, batch, target, mask = next(iter(batches(train_data, sites=site_bag, days=day_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(batch.shape, target.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = ff(batch, mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = ide(target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(batch, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sites = set(test_data.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_batches = 1000\n",
    "max_batch_length = 100  # Maximum number os day-long measurement sequence (of one site) per batch\n",
    "\n",
    "max_seq_length = 24*60 # Maximum number of measurements per site per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 0.."
     ]
    }
   ],
   "source": [
    "# We'll train the network with 10 epochs of a maximum of `max_batches` each\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('TRAIN', end=' ')\n",
    "    for batch_num, batch, target, mask in batches(train_data, sites=site_bag, days=day_bag,\n",
    "                                                  max_batches=max_batches, max_batch_length=max_batch_length):\n",
    "        train(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            if batch_num % 100 == 0:\n",
    "                print(batch_num, end='')\n",
    "            print(\".\", end='')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    cost_val = 0.0\n",
    "    print('TEST', end=' ')\n",
    "    for batch_num, batch, target, mask in batches(test_data, days=set((test_day,)), sites=test_sites):\n",
    "        cost_val += compute_cost(batch, target, mask)\n",
    "        if batch_num % 10 == 0:\n",
    "            print(batch_num, end='')\n",
    "\n",
    "    cost_val = cost_val/(batch_num + 1)\n",
    "    print('')\n",
    "    \n",
    "    print(\"Epoch {} validation cost = {}\".format(epoch + 1, cost_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "companionenv",
   "language": "python",
   "name": "companionenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
